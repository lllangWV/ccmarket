# S3 Storage Reference

Use S3-compatible object storage as package channels.

## Channel Configuration

```toml
[workspace]
channels = ["s3://my-bucket/my-channel", "conda-forge"]
```

## Authentication

Two mutually exclusive methods:

### AWS environment/config (default)

Uses standard AWS credentials when `s3-options` is not specified:

```bash
export AWS_ACCESS_KEY_ID=your-key-id
export AWS_SECRET_ACCESS_KEY=your-secret-key
export AWS_REGION=us-east-1
```

Or via `~/.aws/credentials`:

```ini
[default]
aws_access_key_id = your-key-id
aws_secret_access_key = your-secret-key
region = us-east-1
```

### Pixi credential storage

Store credentials in pixi's auth system:

```bash
pixi auth login s3://my-bucket \
  --s3-access-key-id <KEY_ID> \
  --s3-secret-access-key <SECRET_KEY>
```

## S3 Options

Configure endpoint and region in `pixi.toml`:

```toml
[workspace]
channels = ["s3://my-bucket/my-channel"]

[workspace.s3-options.my-bucket]
endpoint-url = "https://s3.us-east-1.amazonaws.com"
region = "us-east-1"
force-path-style = false
```

## Public Buckets

Public buckets skip authentication. Use HTTPS URLs:

```toml
[workspace]
channels = ["https://my-bucket.s3.amazonaws.com/my-channel"]
```

Required bucket policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject", "s3:ListBucket"],
      "Resource": [
        "arn:aws:s3:::my-bucket",
        "arn:aws:s3:::my-bucket/*"
      ]
    }
  ]
}
```

## S3-Compatible Providers

### MinIO

```toml
[workspace.s3-options.my-bucket]
endpoint-url = "https://minio.example.com"
region = "us-east-1"
force-path-style = true
```

### Cloudflare R2

```toml
[workspace.s3-options.my-bucket]
endpoint-url = "https://<account-id>.r2.cloudflarestorage.com"
region = "auto"
```

### Wasabi

```toml
[workspace.s3-options.my-bucket]
endpoint-url = "https://s3.wasabisys.com"
region = "us-east-1"
```

### Backblaze B2

```toml
[workspace.s3-options.my-bucket]
endpoint-url = "https://s3.us-west-004.backblazeb2.com"
region = "us-west-004"
```

### Google Cloud Storage

```toml
[workspace.s3-options.my-bucket]
endpoint-url = "https://storage.googleapis.com"
region = "auto"
```

### Hetzner Object Storage

```toml
[workspace.s3-options.my-bucket]
endpoint-url = "https://fsn1.your-objectstorage.com"
region = "fsn1"
```

## Uploading Packages

Upload packages to S3 channel:

```bash
pixi upload s3 --bucket my-bucket --channel my-channel my_package.conda
```

After uploading, regenerate repository metadata:

```bash
rattler-index s3://my-bucket/my-channel
```

## Bucket Structure

S3 bucket must follow conda repository structure:

```
my-bucket/
└── my-channel/
    ├── noarch/
    │   ├── repodata.json
    │   └── package-1.0-py_0.conda
    ├── linux-64/
    │   ├── repodata.json
    │   └── package-1.0-py311_0.conda
    ├── osx-arm64/
    │   └── repodata.json
    └── win-64/
        └── repodata.json
```

## CI/CD Integration

### GitHub Actions

```yaml
- uses: prefix-dev/setup-pixi@v0.9.2
  with:
    auth-host: s3://my-bucket
    auth-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    auth-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
```

### Environment variables

```yaml
env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: us-east-1
```

## Troubleshooting

### Debug S3 access

```bash
pixi install -vvv
```

### Test bucket access

```bash
aws s3 ls s3://my-bucket/my-channel/
```

### Verify repodata

```bash
aws s3 cp s3://my-bucket/my-channel/linux-64/repodata.json - | head
```
